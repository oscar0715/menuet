---
title: 推荐算法学习笔记
mathjax: true 
tags:
  - Algorithm
categories:
  - Technology
  - Algorithm
date: 2018-07-20 10:00:00
---
系统推荐的笔记

<!-- more -->

***

# 参考资料放在最前面

1. [矩阵分解](https://time.geekbang.org/column/article/5030) 

<br>
***
<br>

# 矩阵分解
假设用户物品的评分矩阵 A 是 m 乘以 n 维，即一共有 m 个用户，n 个物品。在实际场景中 m 和 n 都会是相对比较大的数字，我们选取一个较小的数字 k ，然后通过算法将 A 矩阵拆分成两个矩阵 U 和 V。U 是 m 乘以 k 维的，V 是 n 乘以 k 维的。

也就是说 $ A\_{m \times n} = U\_{m \times k} V^T\_{n \times k}$

这样的过程就叫做矩阵分解。矩阵分解有很多种方法

## 基础的SVD
SVD是矩阵分解的一种方法，全称是 Singular Value Decomposition，奇异值分解。

矩阵分解在物理上的意义就是把用户和商品投影到一个 k 维空间里，这些维度一般没有业务上的解释，所以把它们当做隐性因子:

- 每一个物品得到一个向量 q ，向量中的元素有正有负，代表物品背后暗藏的一些用户关注的元素，这些元素可能没有业务上的解释。
- 同样每个用户得到一个向量 p，同样这些元素也有正有负，代表着用户对某些因素的偏好
- 物品被关注的因素，用户的偏好，在数量和意义上都是一致的，数量就是人为指定的 k 维度

矩阵分解的结果是:
- 关于用户的向量 $p_u$
- 关于物品的向量 $q_i$
- 计算物品 $i$ 推荐给用户 $u$ 的分数就是 
    $$\hat{r}\_{ui} = p\_u q^T\_i$$

难点在于如何计算出这个 $k$ 维。这是一个机器学习的过程，考虑两个因素：
1. 损失函数
2. 优化算法

SVD 的损失函数: 
$$ min\_{p^\*, q^\*} \sum\_{u,i} (r\_{ui} - p\_u q^T\_i)^2 + \lambda(||q\_i||^2 + ||p\_u||^2) $$
















